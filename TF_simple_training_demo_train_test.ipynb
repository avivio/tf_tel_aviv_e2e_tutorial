{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! rm -rf /tmp/tf_tutorial/logs/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data as usual but add to query a random train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s=r'''SELECT regexp_extract(sample_path, '.*(\\\\..+)$') as suffix,\n",
    "             content,\n",
    "             RAND() >0.1 as is_train\n",
    "      FROM  [bigquery-public-data:github_repos.sample_contents]      \n",
    "      WHERE \n",
    "         content IS NOT NULL\n",
    "          AND content != ''\n",
    "          and length(content) > 1024\n",
    "      HAVING \n",
    "             suffix IS NOT NULL\n",
    "             AND suffix in ('.py','.c','rb')\n",
    "      LIMIT 1000;'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "c = bigquery.Client()\n",
    "query = c.run_sync_query(s)\n",
    "query.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data to 2 csvs according to random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from base64 import urlsafe_b64encode\n",
    "\n",
    "\n",
    "def transform_content(content):\n",
    "    content = content.encode('utf-8')    \n",
    "    content = content[:1024]    \n",
    "    return content\n",
    "\n",
    "with file('/tmp/train.csv','wb') as train_out_file:\n",
    "    with file('/tmp/test.csv','wb') as test_out_file:\n",
    "        w_train = csv.writer(train_out_file, quoting=csv.QUOTE_ALL, delimiter='\\t')\n",
    "        w_test = csv.writer(test_out_file, quoting=csv.QUOTE_ALL, delimiter='\\t')\n",
    "        for language,content, is_train in query.rows:\n",
    "            # decide where to write by the is_train value\n",
    "            w = w_train if is_train else w_test\n",
    "            w.writerow([language,urlsafe_b64encode(transform_content(content))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packaged net building into method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_ITERATIONS = 500\n",
    "MAX_STRING_SIZE = 1024\n",
    "\n",
    "def make_net(input_path, batch_size=BATCH_SIZE):\n",
    "    filenames_queue = tf.train.string_input_producer([input_path])\n",
    "    reader = tf.TextLineReader()\n",
    "    key, value = reader.read(filenames_queue)\n",
    "    default_values=[['UNKNOWN'], ['']]\n",
    "\n",
    "\n",
    "    # decode content\n",
    "    language, b64_content = tf.decode_csv(value,default_values, field_delim='\\t')\n",
    "    content = tf.decode_base64(b64_content)\n",
    "\n",
    "    language_batch_op, content_batch_op = tf.train.shuffle_batch([language,content], \n",
    "                                                                 batch_size=batch_size, \n",
    "                                                                 capacity=1000, \n",
    "                                                                 min_after_dequeue=100)\n",
    "    #Make hash table for langauges\n",
    "    language_keys=['.py','.c','.h']\n",
    "    values=range(1, len(language_keys)+1)\n",
    "    language_codes_table = tf.contrib.lookup.HashTable(\n",
    "        tf.contrib.lookup.KeyValueTensorInitializer(language_keys, values), 0)\n",
    "\n",
    "    #Make embeddings for the characters\n",
    "    bytes = tf.transpose(tf.decode_raw(content_batch_op, tf.uint8))\n",
    "    bytes_embedding_weights = tf.Variable(name=\"embedding_weights\",\n",
    "                                          initial_value=tf.random_uniform(shape=(256, 64),\n",
    "                                                                          minval=-0.1, \n",
    "                                                                          maxval=0.1))\n",
    "    bytes_embedding = tf.nn.embedding_lookup(bytes_embedding_weights, tf.cast(bytes,tf.int32))\n",
    "    embedding_mean = tf.reduce_mean(bytes_embedding,axis=0)\n",
    "\n",
    "    # Convert languages to numeric codes\n",
    "    language_codes_indices = language_codes_table.lookup(language_batch_op)\n",
    "    language_codes_batch_op = tf.one_hot(language_codes_indices, len(language_keys))\n",
    "    dense_weights = tf.get_variable(name='dense_weights',\n",
    "                              shape=[64, len(language_keys)],\n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.Variable(tf.zeros([len(language_keys)]), name='biases')\n",
    "    logits = tf.nn.relu(tf.matmul(embedding_mean, dense_weights) + biases, name='logits')\n",
    "    prediction = tf.argmax(logits, 1)\n",
    "\n",
    "    \n",
    "    batch_loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=language_codes_batch_op)\n",
    "    loss_op = tf.reduce_mean(batch_loss, name='loss')\n",
    "    return prediction, logits, loss_op, content_batch_op\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test batch by running content batch op only on test csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    _,_,_, content_batch_op = make_net('/tmp/test.csv')\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    test_batch = sess.run([content_batch_op])[0] \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "print test_batch.shape    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train as usual but add summaries for both train and test, where for test only run the content batch op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.09824\n",
      "Test loss: 1.09412\n",
      "Train loss: 1.09699\n",
      "Test loss: 1.09377\n",
      "Train loss: 1.09531\n",
      "Test loss: 1.09323\n",
      "Train loss: 1.09262\n",
      "Test loss: 1.08845\n",
      "Train loss: 1.09379\n",
      "Test loss: 1.09083\n",
      "Train loss: 1.0868\n",
      "Test loss: 1.0865\n",
      "Train loss: 1.08976\n",
      "Test loss: 1.086\n",
      "Train loss: 1.08051\n",
      "Test loss: 1.0831\n",
      "Train loss: 1.07761\n",
      "Test loss: 1.07878\n",
      "Train loss: 1.07296\n",
      "Test loss: 1.07903\n",
      "Train loss: 1.07852\n",
      "Test loss: 1.07742\n",
      "Train loss: 1.06761\n",
      "Test loss: 1.07229\n",
      "Train loss: 1.06623\n",
      "Test loss: 1.06916\n",
      "Train loss: 1.06464\n",
      "Test loss: 1.06839\n",
      "Train loss: 1.06584\n",
      "Test loss: 1.0695\n",
      "Train loss: 1.06328\n",
      "Test loss: 1.06484\n",
      "Train loss: 1.05362\n",
      "Test loss: 1.0598\n",
      "Train loss: 1.04758\n",
      "Test loss: 1.05528\n",
      "Train loss: 1.05838\n",
      "Test loss: 1.05722\n",
      "Train loss: 1.04954\n",
      "Test loss: 1.05279\n",
      "Train loss: 1.04881\n",
      "Test loss: 1.04928\n",
      "Train loss: 1.04094\n",
      "Test loss: 1.04929\n",
      "Train loss: 1.04054\n",
      "Test loss: 1.03907\n",
      "Train loss: 1.03294\n",
      "Test loss: 1.03194\n",
      "Train loss: 1.0342\n",
      "Test loss: 1.03755\n",
      "Train loss: 1.03126\n",
      "Test loss: 1.0397\n",
      "Train loss: 1.02664\n",
      "Test loss: 1.03407\n",
      "Train loss: 1.01893\n",
      "Test loss: 1.02935\n",
      "Train loss: 1.02033\n",
      "Test loss: 1.03478\n",
      "Train loss: 1.01662\n",
      "Test loss: 1.02414\n",
      "Train loss: 1.0055\n",
      "Test loss: 1.02333\n",
      "Train loss: 1.01031\n",
      "Test loss: 1.01957\n",
      "Train loss: 1.01411\n",
      "Test loss: 1.01995\n",
      "Train loss: 0.998961\n",
      "Test loss: 1.01513\n",
      "Train loss: 0.998439\n",
      "Test loss: 1.01621\n",
      "Train loss: 0.991827\n",
      "Test loss: 1.00886\n",
      "Train loss: 0.999568\n",
      "Test loss: 1.01209\n",
      "Train loss: 0.997984\n",
      "Test loss: 0.998684\n",
      "Train loss: 0.982515\n",
      "Test loss: 0.996083\n",
      "Train loss: 0.983916\n",
      "Test loss: 0.999759\n",
      "Train loss: 0.97807\n",
      "Test loss: 0.995347\n",
      "Train loss: 0.974349\n",
      "Test loss: 0.99378\n",
      "Train loss: 0.979309\n",
      "Test loss: 0.993451\n",
      "Train loss: 0.963675\n",
      "Test loss: 0.986705\n",
      "Train loss: 0.96558\n",
      "Test loss: 0.982441\n",
      "Train loss: 0.965009\n",
      "Test loss: 0.990962\n",
      "Train loss: 0.961431\n",
      "Test loss: 0.978277\n",
      "Train loss: 0.953323\n",
      "Test loss: 0.972712\n",
      "Train loss: 0.959245\n",
      "Test loss: 0.973593\n",
      "Train loss: 0.962911\n",
      "Test loss: 0.963875\n",
      "Train loss: 0.950835\n",
      "Test loss: 0.963845\n",
      "Train loss: 0.945776\n",
      "Test loss: 0.963652\n",
      "Train loss: 0.947168\n",
      "Test loss: 0.952433\n",
      "Train loss: 0.934767\n",
      "Test loss: 0.962784\n",
      "Train loss: 0.938207\n",
      "Test loss: 0.956635\n",
      "Train loss: 0.938182\n",
      "Test loss: 0.955283\n",
      "Train loss: 0.922621\n",
      "Test loss: 0.954966\n",
      "Train loss: 0.926937\n",
      "Test loss: 0.945377\n",
      "Train loss: 0.921575\n",
      "Test loss: 0.944809\n",
      "Train loss: 0.917592\n",
      "Test loss: 0.935113\n",
      "Train loss: 0.903468\n",
      "Test loss: 0.942012\n",
      "Train loss: 0.912636\n",
      "Test loss: 0.931648\n",
      "Train loss: 0.901944\n",
      "Test loss: 0.93218\n",
      "Train loss: 0.899621\n",
      "Test loss: 0.927141\n",
      "Train loss: 0.906528\n",
      "Test loss: 0.932034\n",
      "Train loss: 0.900199\n",
      "Test loss: 0.918319\n",
      "Train loss: 0.901808\n",
      "Test loss: 0.909856\n",
      "Train loss: 0.886881\n",
      "Test loss: 0.914486\n",
      "Train loss: 0.88534\n",
      "Test loss: 0.919819\n",
      "Train loss: 0.885768\n",
      "Test loss: 0.920473\n",
      "Train loss: 0.885502\n",
      "Test loss: 0.906666\n",
      "Train loss: 0.879813\n",
      "Test loss: 0.912567\n",
      "Train loss: 0.873679\n",
      "Test loss: 0.898935\n",
      "Train loss: 0.874397\n",
      "Test loss: 0.902406\n",
      "Train loss: 0.871435\n",
      "Test loss: 0.899141\n",
      "Train loss: 0.865497\n",
      "Test loss: 0.910292\n",
      "Train loss: 0.862935\n",
      "Test loss: 0.892703\n",
      "Train loss: 0.854358\n",
      "Test loss: 0.893098\n",
      "Train loss: 0.852073\n",
      "Test loss: 0.890633\n",
      "Train loss: 0.85351\n",
      "Test loss: 0.89427\n",
      "Train loss: 0.848953\n",
      "Test loss: 0.885818\n",
      "Train loss: 0.853968\n",
      "Test loss: 0.881015\n",
      "Train loss: 0.839683\n",
      "Test loss: 0.879626\n",
      "Train loss: 0.826591\n",
      "Test loss: 0.877512\n",
      "Train loss: 0.843365\n",
      "Test loss: 0.871649\n",
      "Train loss: 0.840261\n",
      "Test loss: 0.882064\n",
      "Train loss: 0.834249\n",
      "Test loss: 0.872913\n",
      "Train loss: 0.828592\n",
      "Test loss: 0.871485\n",
      "Train loss: 0.823544\n",
      "Test loss: 0.855568\n",
      "Train loss: 0.824492\n",
      "Test loss: 0.857837\n",
      "Train loss: 0.813429\n",
      "Test loss: 0.858903\n",
      "Train loss: 0.816991\n",
      "Test loss: 0.854517\n",
      "Train loss: 0.808022\n",
      "Test loss: 0.853565\n",
      "Train loss: 0.807902\n",
      "Test loss: 0.846226\n",
      "Train loss: 0.8033\n",
      "Test loss: 0.858709\n",
      "Train loss: 0.796842\n",
      "Test loss: 0.853015\n",
      "Train loss: 0.794961\n",
      "Test loss: 0.847579\n",
      "Train loss: 0.791015\n",
      "Test loss: 0.845547\n",
      "Train loss: 0.793128\n",
      "Test loss: 0.841705\n",
      "Train loss: 0.785637\n",
      "Test loss: 0.83589\n",
      "Train loss: 0.777393\n",
      "Test loss: 0.840344\n",
      "Train loss: 0.785544\n",
      "Test loss: 0.837942\n",
      "Train loss: 0.787001\n",
      "Test loss: 0.842343\n",
      "Train loss: 0.773706\n",
      "Test loss: 0.836551\n",
      "Train loss: 0.770895\n",
      "Test loss: 0.831106\n",
      "Train loss: 0.780179\n",
      "Test loss: 0.839757\n",
      "Train loss: 0.771342\n",
      "Test loss: 0.835404\n",
      "Train loss: 0.754203\n",
      "Test loss: 0.837538\n",
      "Train loss: 0.75904\n",
      "Test loss: 0.82051\n",
      "Train loss: 0.767757\n",
      "Test loss: 0.823065\n",
      "Train loss: 0.76003\n",
      "Test loss: 0.817493\n",
      "Train loss: 0.746722\n",
      "Test loss: 0.82004\n",
      "Train loss: 0.748218\n",
      "Test loss: 0.819037\n",
      "Train loss: 0.753219\n",
      "Test loss: 0.833125\n",
      "Train loss: 0.743533\n",
      "Test loss: 0.833799\n",
      "Train loss: 0.73007\n",
      "Test loss: 0.83252\n",
      "Train loss: 0.746994\n",
      "Test loss: 0.799226\n",
      "Train loss: 0.729633\n",
      "Test loss: 0.828998\n",
      "Train loss: 0.745709\n",
      "Test loss: 0.807636\n",
      "Train loss: 0.726873\n",
      "Test loss: 0.816306\n",
      "Train loss: 0.726319\n",
      "Test loss: 0.811366\n",
      "Train loss: 0.735205\n",
      "Test loss: 0.807326\n",
      "Train loss: 0.720015\n",
      "Test loss: 0.801784\n",
      "Train loss: 0.744491\n",
      "Test loss: 0.822779\n",
      "Train loss: 0.717055\n",
      "Test loss: 0.814898\n",
      "Train loss: 0.722283\n",
      "Test loss: 0.785417\n",
      "Train loss: 0.721392\n",
      "Test loss: 0.820072\n",
      "Train loss: 0.705407\n",
      "Test loss: 0.78711\n",
      "Train loss: 0.718257\n",
      "Test loss: 0.796552\n",
      "Train loss: 0.712405\n",
      "Test loss: 0.807718\n",
      "Train loss: 0.714426\n",
      "Test loss: 0.793799\n",
      "Train loss: 0.705841\n",
      "Test loss: 0.80945\n",
      "Train loss: 0.702954\n",
      "Test loss: 0.787281\n",
      "Train loss: 0.708329\n",
      "Test loss: 0.766952\n",
      "Train loss: 0.71394\n",
      "Test loss: 0.802264\n",
      "Train loss: 0.690985\n",
      "Test loss: 0.798228\n",
      "Train loss: 0.703751\n",
      "Test loss: 0.812081\n",
      "Train loss: 0.686968\n",
      "Test loss: 0.785187\n",
      "Train loss: 0.70502\n",
      "Test loss: 0.766419\n",
      "Train loss: 0.690331\n",
      "Test loss: 0.795952\n",
      "Train loss: 0.680979\n",
      "Test loss: 0.799786\n",
      "Train loss: 0.68679\n",
      "Test loss: 0.80372\n",
      "Train loss: 0.686732\n",
      "Test loss: 0.797484\n",
      "Train loss: 0.683905\n",
      "Test loss: 0.790763\n",
      "Train loss: 0.678076\n",
      "Test loss: 0.776123\n",
      "Train loss: 0.676646\n",
      "Test loss: 0.76262\n",
      "Train loss: 0.672894\n",
      "Test loss: 0.775715\n",
      "Train loss: 0.679426\n",
      "Test loss: 0.800086\n",
      "Train loss: 0.662362\n",
      "Test loss: 0.783294\n",
      "Train loss: 0.656049\n",
      "Test loss: 0.771687\n",
      "Train loss: 0.67177\n",
      "Test loss: 0.789133\n",
      "Train loss: 0.675238\n",
      "Test loss: 0.765129\n",
      "Train loss: 0.664319\n",
      "Test loss: 0.787653\n",
      "Train loss: 0.665981\n",
      "Test loss: 0.77662\n",
      "Train loss: 0.660472\n",
      "Test loss: 0.79315\n",
      "Train loss: 0.664736\n",
      "Test loss: 0.771562\n",
      "Train loss: 0.644254\n",
      "Test loss: 0.764681\n",
      "Train loss: 0.65734\n",
      "Test loss: 0.764068\n",
      "Train loss: 0.647113\n",
      "Test loss: 0.763901\n",
      "Train loss: 0.644478\n",
      "Test loss: 0.780864\n",
      "Train loss: 0.630407\n",
      "Test loss: 0.759145\n",
      "Train loss: 0.655531\n",
      "Test loss: 0.779554\n",
      "Train loss: 0.64733\n",
      "Test loss: 0.744905\n",
      "Train loss: 0.647826\n",
      "Test loss: 0.748438\n",
      "Train loss: 0.645021\n",
      "Test loss: 0.763684\n",
      "Train loss: 0.637383\n",
      "Test loss: 0.754238\n",
      "Train loss: 0.643328\n",
      "Test loss: 0.783304\n",
      "Train loss: 0.647359\n",
      "Test loss: 0.789557\n",
      "Train loss: 0.616323\n",
      "Test loss: 0.763865\n",
      "Train loss: 0.624308\n",
      "Test loss: 0.780162\n",
      "Train loss: 0.617559\n",
      "Test loss: 0.753799\n",
      "Train loss: 0.633452\n",
      "Test loss: 0.773833\n",
      "Train loss: 0.615336\n",
      "Test loss: 0.762059\n",
      "Train loss: 0.621798\n",
      "Test loss: 0.735702\n",
      "Train loss: 0.591866\n",
      "Test loss: 0.78071\n",
      "Train loss: 0.61404\n",
      "Test loss: 0.812415\n",
      "Train loss: 0.600807\n",
      "Test loss: 0.808872\n",
      "Train loss: 0.610635\n",
      "Test loss: 0.777628\n",
      "Train loss: 0.613937\n",
      "Test loss: 0.77352\n",
      "Train loss: 0.618503\n",
      "Test loss: 0.753597\n",
      "Train loss: 0.608094\n",
      "Test loss: 0.787391\n",
      "Train loss: 0.627181\n",
      "Test loss: 0.78837\n",
      "Train loss: 0.591643\n",
      "Test loss: 0.747705\n",
      "Train loss: 0.587923\n",
      "Test loss: 0.791158\n",
      "Train loss: 0.606531\n",
      "Test loss: 0.79349\n",
      "Train loss: 0.606034\n",
      "Test loss: 0.77383\n",
      "Train loss: 0.584719\n",
      "Test loss: 0.752872\n",
      "Train loss: 0.610212\n",
      "Test loss: 0.769892\n",
      "Train loss: 0.584488\n",
      "Test loss: 0.812889\n",
      "Train loss: 0.571613\n",
      "Test loss: 0.760684\n",
      "Train loss: 0.594298\n",
      "Test loss: 0.788451\n",
      "Train loss: 0.613242\n",
      "Test loss: 0.768464\n",
      "Train loss: 0.574903\n",
      "Test loss: 0.745908\n",
      "Train loss: 0.60161\n",
      "Test loss: 0.738315\n",
      "Train loss: 0.575729\n",
      "Test loss: 0.738443\n",
      "Train loss: 0.561199\n",
      "Test loss: 0.772702\n",
      "Train loss: 0.583296\n",
      "Test loss: 0.77319\n",
      "Train loss: 0.57556\n",
      "Test loss: 0.777845\n",
      "Train loss: 0.574125\n",
      "Test loss: 0.765391\n",
      "Train loss: 0.610469\n",
      "Test loss: 0.738153\n",
      "Train loss: 0.565999\n",
      "Test loss: 0.799082\n",
      "Train loss: 0.551111\n",
      "Test loss: 0.803095\n",
      "Train loss: 0.562744\n",
      "Test loss: 0.752986\n",
      "Train loss: 0.555105\n",
      "Test loss: 0.730358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.549408\n",
      "Test loss: 0.818002\n",
      "Train loss: 0.568108\n",
      "Test loss: 0.779191\n",
      "Train loss: 0.565863\n",
      "Test loss: 0.794401\n",
      "Train loss: 0.57149\n",
      "Test loss: 0.73776\n",
      "Train loss: 0.546763\n",
      "Test loss: 0.706812\n",
      "Train loss: 0.556873\n",
      "Test loss: 0.755346\n",
      "Train loss: 0.537296\n",
      "Test loss: 0.767871\n",
      "Train loss: 0.556796\n",
      "Test loss: 0.759543\n",
      "Train loss: 0.527063\n",
      "Test loss: 0.749928\n",
      "Train loss: 0.530049\n",
      "Test loss: 0.756794\n",
      "Train loss: 0.565602\n",
      "Test loss: 0.76086\n",
      "Train loss: 0.518429\n",
      "Test loss: 0.753129\n",
      "Train loss: 0.533867\n",
      "Test loss: 0.793632\n",
      "Train loss: 0.542042\n",
      "Test loss: 0.795173\n",
      "Train loss: 0.531332\n",
      "Test loss: 0.777279\n",
      "Train loss: 0.5405\n",
      "Test loss: 0.810058\n",
      "Train loss: 0.509232\n",
      "Test loss: 0.74187\n",
      "Train loss: 0.539004\n",
      "Test loss: 0.789165\n",
      "Train loss: 0.530973\n",
      "Test loss: 0.785293\n",
      "Train loss: 0.541583\n",
      "Test loss: 0.79798\n",
      "Train loss: 0.539074\n",
      "Test loss: 0.731506\n",
      "Train loss: 0.544586\n",
      "Test loss: 0.789455\n",
      "Train loss: 0.533756\n",
      "Test loss: 0.790195\n",
      "Train loss: 0.534239\n",
      "Test loss: 0.765993\n",
      "Train loss: 0.516432\n",
      "Test loss: 0.763985\n",
      "Train loss: 0.514116\n",
      "Test loss: 0.790022\n",
      "Train loss: 0.501512\n",
      "Test loss: 0.786774\n",
      "Train loss: 0.498994\n",
      "Test loss: 0.801146\n",
      "Train loss: 0.500706\n",
      "Test loss: 0.769776\n",
      "Train loss: 0.535958\n",
      "Test loss: 0.785008\n",
      "Train loss: 0.494028\n",
      "Test loss: 0.744335\n",
      "Train loss: 0.5048\n",
      "Test loss: 0.772971\n",
      "Train loss: 0.518697\n",
      "Test loss: 0.807742\n",
      "Train loss: 0.501475\n",
      "Test loss: 0.793673\n",
      "Train loss: 0.492928\n",
      "Test loss: 0.84731\n",
      "Train loss: 0.498805\n",
      "Test loss: 0.81196\n",
      "Train loss: 0.503929\n",
      "Test loss: 0.812313\n",
      "Train loss: 0.529175\n",
      "Test loss: 0.753218\n",
      "Train loss: 0.498633\n",
      "Test loss: 0.742008\n",
      "Train loss: 0.498432\n",
      "Test loss: 0.74163\n",
      "Train loss: 0.493641\n",
      "Test loss: 0.756797\n",
      "Train loss: 0.479118\n",
      "Test loss: 0.768662\n",
      "Train loss: 0.504333\n",
      "Test loss: 0.841805\n",
      "Train loss: 0.491442\n",
      "Test loss: 0.757805\n",
      "Train loss: 0.491129\n",
      "Test loss: 0.779957\n",
      "Train loss: 0.489354\n",
      "Test loss: 0.784029\n",
      "Train loss: 0.522618\n",
      "Test loss: 0.848275\n",
      "Train loss: 0.452803\n",
      "Test loss: 0.822039\n",
      "Train loss: 0.483837\n",
      "Test loss: 0.761195\n",
      "Train loss: 0.484723\n",
      "Test loss: 0.794666\n",
      "Train loss: 0.511431\n",
      "Test loss: 0.818838\n",
      "Train loss: 0.492828\n",
      "Test loss: 0.86353\n",
      "Train loss: 0.470464\n",
      "Test loss: 0.76907\n",
      "Train loss: 0.477705\n",
      "Test loss: 0.817697\n",
      "Train loss: 0.479257\n",
      "Test loss: 0.730922\n",
      "Train loss: 0.485218\n",
      "Test loss: 0.760452\n",
      "Train loss: 0.436871\n",
      "Test loss: 0.833557\n",
      "Train loss: 0.450371\n",
      "Test loss: 0.826113\n",
      "Train loss: 0.500125\n",
      "Test loss: 0.833973\n",
      "Train loss: 0.451615\n",
      "Test loss: 0.84133\n",
      "Train loss: 0.470306\n",
      "Test loss: 0.801355\n",
      "Train loss: 0.479548\n",
      "Test loss: 0.805207\n",
      "Train loss: 0.44512\n",
      "Test loss: 0.787325\n",
      "Train loss: 0.491619\n",
      "Test loss: 0.828488\n",
      "Train loss: 0.477514\n",
      "Test loss: 0.755945\n",
      "Train loss: 0.42904\n",
      "Test loss: 0.860549\n",
      "Train loss: 0.46075\n",
      "Test loss: 0.738129\n",
      "Train loss: 0.455722\n",
      "Test loss: 0.855151\n",
      "Train loss: 0.460455\n",
      "Test loss: 0.888386\n",
      "Train loss: 0.488999\n",
      "Test loss: 0.759486\n",
      "Train loss: 0.426904\n",
      "Test loss: 0.860675\n",
      "Train loss: 0.460525\n",
      "Test loss: 0.813093\n",
      "Train loss: 0.433876\n",
      "Test loss: 0.818258\n",
      "Train loss: 0.441084\n",
      "Test loss: 0.842068\n",
      "Train loss: 0.470398\n",
      "Test loss: 0.862032\n",
      "Train loss: 0.417829\n",
      "Test loss: 0.81774\n",
      "Train loss: 0.436591\n",
      "Test loss: 0.757943\n",
      "Train loss: 0.44374\n",
      "Test loss: 0.86074\n",
      "Train loss: 0.44053\n",
      "Test loss: 0.754151\n",
      "Train loss: 0.444444\n",
      "Test loss: 0.747291\n",
      "Train loss: 0.430919\n",
      "Test loss: 0.759355\n",
      "Train loss: 0.427367\n",
      "Test loss: 0.755762\n",
      "Train loss: 0.445245\n",
      "Test loss: 0.904972\n",
      "Train loss: 0.460765\n",
      "Test loss: 0.814591\n",
      "Train loss: 0.420973\n",
      "Test loss: 0.835057\n",
      "Train loss: 0.430318\n",
      "Test loss: 0.873437\n",
      "Train loss: 0.459378\n",
      "Test loss: 0.888132\n",
      "Train loss: 0.429227\n",
      "Test loss: 0.908587\n",
      "Train loss: 0.422554\n",
      "Test loss: 0.835721\n",
      "Train loss: 0.408739\n",
      "Test loss: 0.85855\n",
      "Train loss: 0.421523\n",
      "Test loss: 0.892529\n",
      "Train loss: 0.41093\n",
      "Test loss: 0.772605\n",
      "Train loss: 0.411156\n",
      "Test loss: 0.838839\n",
      "Train loss: 0.448376\n",
      "Test loss: 0.788433\n",
      "Train loss: 0.416785\n",
      "Test loss: 0.818707\n",
      "Train loss: 0.381075\n",
      "Test loss: 0.783006\n",
      "Train loss: 0.420433\n",
      "Test loss: 0.816601\n",
      "Train loss: 0.424358\n",
      "Test loss: 0.801682\n",
      "Train loss: 0.42711\n",
      "Test loss: 0.782908\n",
      "Train loss: 0.430544\n",
      "Test loss: 0.786076\n",
      "Train loss: 0.426142\n",
      "Test loss: 0.818534\n",
      "Train loss: 0.425137\n",
      "Test loss: 0.896213\n",
      "Train loss: 0.413571\n",
      "Test loss: 0.871755\n",
      "Train loss: 0.411963\n",
      "Test loss: 0.833222\n",
      "Train loss: 0.384507\n",
      "Test loss: 0.842059\n",
      "Train loss: 0.416073\n",
      "Test loss: 0.787554\n",
      "Train loss: 0.395763\n",
      "Test loss: 0.894232\n",
      "Train loss: 0.401318\n",
      "Test loss: 0.843478\n",
      "Train loss: 0.429745\n",
      "Test loss: 0.806968\n",
      "Train loss: 0.386051\n",
      "Test loss: 0.901058\n",
      "Train loss: 0.418229\n",
      "Test loss: 0.762708\n",
      "Train loss: 0.396855\n",
      "Test loss: 0.783784\n",
      "Train loss: 0.365529\n",
      "Test loss: 0.792945\n",
      "Train loss: 0.430997\n",
      "Test loss: 0.843902\n",
      "Train loss: 0.386236\n",
      "Test loss: 0.944256\n",
      "Train loss: 0.391914\n",
      "Test loss: 0.848071\n",
      "Train loss: 0.399805\n",
      "Test loss: 0.908807\n",
      "Train loss: 0.388919\n",
      "Test loss: 0.836622\n",
      "Train loss: 0.385122\n",
      "Test loss: 0.834037\n",
      "Train loss: 0.356099\n",
      "Test loss: 0.803954\n",
      "Train loss: 0.374398\n",
      "Test loss: 0.771746\n",
      "Train loss: 0.390241\n",
      "Test loss: 0.854416\n",
      "Train loss: 0.397776\n",
      "Test loss: 0.902591\n",
      "Train loss: 0.344552\n",
      "Test loss: 0.910843\n",
      "Train loss: 0.399974\n",
      "Test loss: 0.886603\n",
      "Train loss: 0.383705\n",
      "Test loss: 0.978163\n",
      "Train loss: 0.406759\n",
      "Test loss: 0.942015\n",
      "Train loss: 0.363367\n",
      "Test loss: 0.878112\n",
      "Train loss: 0.388218\n",
      "Test loss: 0.886936\n",
      "Train loss: 0.36346\n",
      "Test loss: 0.925449\n",
      "Train loss: 0.374836\n",
      "Test loss: 0.850748\n",
      "Train loss: 0.425889\n",
      "Test loss: 0.956144\n",
      "Train loss: 0.376669\n",
      "Test loss: 0.924893\n",
      "Train loss: 0.411963\n",
      "Test loss: 0.890718\n",
      "Train loss: 0.367734\n",
      "Test loss: 0.923023\n",
      "Train loss: 0.41271\n",
      "Test loss: 0.947647\n",
      "Train loss: 0.362723\n",
      "Test loss: 0.869474\n",
      "Train loss: 0.373977\n",
      "Test loss: 0.863678\n",
      "Train loss: 0.393949\n",
      "Test loss: 0.94967\n",
      "Train loss: 0.342854\n",
      "Test loss: 1.01952\n",
      "Train loss: 0.33417\n",
      "Test loss: 0.955659\n",
      "Train loss: 0.399806\n",
      "Test loss: 0.871962\n",
      "Train loss: 0.390362\n",
      "Test loss: 0.939766\n",
      "Train loss: 0.368149\n",
      "Test loss: 0.939163\n",
      "Train loss: 0.355974\n",
      "Test loss: 0.915399\n",
      "Train loss: 0.367652\n",
      "Test loss: 0.922848\n",
      "Train loss: 0.329804\n",
      "Test loss: 0.862994\n",
      "Train loss: 0.434904\n",
      "Test loss: 1.02544\n",
      "Train loss: 0.353269\n",
      "Test loss: 0.873618\n",
      "Train loss: 0.394028\n",
      "Test loss: 0.9283\n",
      "Train loss: 0.362297\n",
      "Test loss: 0.914447\n",
      "Train loss: 0.333311\n",
      "Test loss: 0.873666\n",
      "Train loss: 0.321207\n",
      "Test loss: 0.941311\n",
      "Train loss: 0.354664\n",
      "Test loss: 0.860383\n",
      "Train loss: 0.361858\n",
      "Test loss: 0.921759\n",
      "Train loss: 0.322261\n",
      "Test loss: 0.962225\n",
      "Train loss: 0.336795\n",
      "Test loss: 0.891938\n",
      "Train loss: 0.380284\n",
      "Test loss: 0.887868\n",
      "Train loss: 0.332971\n",
      "Test loss: 0.935735\n",
      "Train loss: 0.357303\n",
      "Test loss: 0.990407\n",
      "Train loss: 0.327769\n",
      "Test loss: 1.00113\n",
      "Train loss: 0.314629\n",
      "Test loss: 0.943388\n",
      "Train loss: 0.326305\n",
      "Test loss: 0.917461\n",
      "Train loss: 0.3179\n",
      "Test loss: 0.948918\n",
      "Train loss: 0.342411\n",
      "Test loss: 0.914534\n",
      "Train loss: 0.349185\n",
      "Test loss: 0.977398\n",
      "Train loss: 0.311351\n",
      "Test loss: 0.834765\n",
      "Train loss: 0.353731\n",
      "Test loss: 0.922616\n",
      "Train loss: 0.331279\n",
      "Test loss: 0.902667\n",
      "Train loss: 0.33304\n",
      "Test loss: 0.976018\n",
      "Train loss: 0.348857\n",
      "Test loss: 0.987208\n",
      "Train loss: 0.343775\n",
      "Test loss: 1.05834\n",
      "Train loss: 0.359025\n",
      "Test loss: 0.930786\n",
      "Train loss: 0.361251\n",
      "Test loss: 0.901332\n",
      "Train loss: 0.318627\n",
      "Test loss: 0.985421\n",
      "Train loss: 0.324038\n",
      "Test loss: 0.947706\n",
      "Train loss: 0.370901\n",
      "Test loss: 0.906488\n",
      "Train loss: 0.321184\n",
      "Test loss: 1.08998\n",
      "Train loss: 0.31878\n",
      "Test loss: 0.919693\n",
      "Train loss: 0.279251\n",
      "Test loss: 0.991795\n",
      "Train loss: 0.339385\n",
      "Test loss: 0.871608\n",
      "Train loss: 0.322807\n",
      "Test loss: 0.933724\n",
      "Train loss: 0.348419\n",
      "Test loss: 0.901329\n",
      "Train loss: 0.328397\n",
      "Test loss: 1.01674\n",
      "Train loss: 0.328043\n",
      "Test loss: 0.95384\n",
      "Train loss: 0.298545\n",
      "Test loss: 1.01019\n",
      "Train loss: 0.281682\n",
      "Test loss: 0.943287\n",
      "Train loss: 0.333357\n",
      "Test loss: 0.907265\n",
      "Train loss: 0.290833\n",
      "Test loss: 0.891428\n",
      "Train loss: 0.351192\n",
      "Test loss: 1.00968\n",
      "Train loss: 0.338012\n",
      "Test loss: 1.0764\n",
      "Train loss: 0.291622\n",
      "Test loss: 0.882513\n",
      "Train loss: 0.346481\n",
      "Test loss: 0.961624\n",
      "Train loss: 0.321202\n",
      "Test loss: 0.823813\n",
      "Train loss: 0.309112\n",
      "Test loss: 0.933221\n",
      "Train loss: 0.288525\n",
      "Test loss: 0.974111\n",
      "Train loss: 0.312817\n",
      "Test loss: 0.966546\n",
      "Train loss: 0.341796\n",
      "Test loss: 0.950227\n",
      "Train loss: 0.33188\n",
      "Test loss: 0.967238\n",
      "Train loss: 0.336203\n",
      "Test loss: 0.971479\n",
      "Train loss: 0.295349\n",
      "Test loss: 0.946387\n",
      "Train loss: 0.322847\n",
      "Test loss: 0.847684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.280596\n",
      "Test loss: 0.79832\n",
      "Train loss: 0.364241\n",
      "Test loss: 0.897583\n",
      "Train loss: 0.341997\n",
      "Test loss: 1.06486\n",
      "Train loss: 0.312194\n",
      "Test loss: 0.914403\n",
      "Train loss: 0.309416\n",
      "Test loss: 1.02652\n",
      "Train loss: 0.275949\n",
      "Test loss: 1.03842\n",
      "Train loss: 0.34881\n",
      "Test loss: 1.04011\n",
      "Train loss: 0.32238\n",
      "Test loss: 1.02425\n",
      "Train loss: 0.354302\n",
      "Test loss: 1.14068\n",
      "Train loss: 0.265619\n",
      "Test loss: 0.863553\n",
      "Train loss: 0.342802\n",
      "Test loss: 1.12241\n",
      "Train loss: 0.295224\n",
      "Test loss: 0.996993\n",
      "Train loss: 0.294705\n",
      "Test loss: 0.959664\n",
      "Train loss: 0.312817\n",
      "Test loss: 1.16316\n",
      "Train loss: 0.304694\n",
      "Test loss: 1.02184\n",
      "Train loss: 0.317895\n",
      "Test loss: 1.05181\n",
      "Train loss: 0.30572\n",
      "Test loss: 1.05307\n",
      "Train loss: 0.267944\n",
      "Test loss: 1.00639\n",
      "Train loss: 0.319356\n",
      "Test loss: 1.14049\n",
      "Train loss: 0.298283\n",
      "Test loss: 0.864371\n",
      "Train loss: 0.2961\n",
      "Test loss: 1.14088\n",
      "Train loss: 0.285131\n",
      "Test loss: 0.915367\n",
      "Train loss: 0.303817\n",
      "Test loss: 1.05664\n",
      "Train loss: 0.280475\n",
      "Test loss: 0.97179\n",
      "Train loss: 0.261344\n",
      "Test loss: 1.19679\n",
      "Train loss: 0.331052\n",
      "Test loss: 0.897954\n",
      "Train loss: 0.310902\n",
      "Test loss: 1.02351\n",
      "Train loss: 0.328197\n",
      "Test loss: 0.934749\n",
      "Train loss: 0.270614\n",
      "Test loss: 1.05025\n",
      "Train loss: 0.298693\n",
      "Test loss: 1.10723\n",
      "Train loss: 0.272474\n",
      "Test loss: 1.04786\n",
      "Train loss: 0.302242\n",
      "Test loss: 1.11627\n",
      "Train loss: 0.30242\n",
      "Test loss: 0.834364\n",
      "Train loss: 0.259888\n",
      "Test loss: 1.0646\n",
      "Train loss: 0.322849\n",
      "Test loss: 1.01752\n",
      "Train loss: 0.279354\n",
      "Test loss: 1.10265\n",
      "Train loss: 0.273279\n",
      "Test loss: 0.952089\n",
      "Train loss: 0.324954\n",
      "Test loss: 1.0197\n",
      "Train loss: 0.294972\n",
      "Test loss: 1.00456\n",
      "Train loss: 0.272221\n",
      "Test loss: 1.00079\n",
      "Train loss: 0.272002\n",
      "Test loss: 1.09889\n",
      "Train loss: 0.295731\n",
      "Test loss: 1.09338\n",
      "Train loss: 0.299837\n",
      "Test loss: 1.20656\n",
      "Train loss: 0.298701\n",
      "Test loss: 0.998724\n",
      "Train loss: 0.321545\n",
      "Test loss: 1.17412\n",
      "Train loss: 0.294817\n",
      "Test loss: 1.15905\n",
      "Train loss: 0.320158\n",
      "Test loss: 1.08888\n",
      "Train loss: 0.284786\n",
      "Test loss: 0.942695\n",
      "Train loss: 0.266984\n",
      "Test loss: 1.06002\n",
      "Train loss: 0.300793\n",
      "Test loss: 1.09193\n",
      "Train loss: 0.276014\n",
      "Test loss: 1.15846\n",
      "Train loss: 0.259051\n",
      "Test loss: 1.15416\n",
      "Train loss: 0.273387\n",
      "Test loss: 1.09653\n",
      "Train loss: 0.277339\n",
      "Test loss: 1.13272\n",
      "Train loss: 0.252848\n",
      "Test loss: 0.998567\n",
      "Train loss: 0.283271\n",
      "Test loss: 1.10919\n",
      "Train loss: 0.279862\n",
      "Test loss: 0.98714\n",
      "Train loss: 0.270527\n",
      "Test loss: 1.09752\n",
      "Train loss: 0.308913\n",
      "Test loss: 1.14309\n",
      "Train loss: 0.272287\n",
      "Test loss: 1.16443\n",
      "Train loss: 0.261008\n",
      "Test loss: 1.01437\n",
      "Train loss: 0.255012\n",
      "Test loss: 1.1876\n",
      "Train loss: 0.245514\n",
      "Test loss: 1.03218\n",
      "Train loss: 0.330633\n",
      "Test loss: 1.0259\n",
      "Train loss: 0.294777\n",
      "Test loss: 1.09655\n",
      "Train loss: 0.253351\n",
      "Test loss: 1.13064\n",
      "Train loss: 0.276091\n",
      "Test loss: 0.963835\n",
      "Train loss: 0.285812\n",
      "Test loss: 1.14047\n",
      "Train loss: 0.287625\n",
      "Test loss: 1.1195\n",
      "Train loss: 0.286152\n",
      "Test loss: 1.18856\n",
      "Train loss: 0.241888\n",
      "Test loss: 1.15793\n",
      "Train loss: 0.262613\n",
      "Test loss: 1.0185\n",
      "Train loss: 0.241424\n",
      "Test loss: 1.00072\n",
      "Train loss: 0.242911\n",
      "Test loss: 1.12054\n",
      "Train loss: 0.292613\n",
      "Test loss: 1.10161\n",
      "Train loss: 0.265154\n",
      "Test loss: 1.08137\n",
      "Train loss: 0.238885\n",
      "Test loss: 0.949946\n",
      "Train loss: 0.256625\n",
      "Test loss: 1.12243\n",
      "Train loss: 0.330361\n",
      "Test loss: 1.1159\n",
      "Train loss: 0.277142\n",
      "Test loss: 1.12802\n",
      "Train loss: 0.276096\n",
      "Test loss: 0.947158\n",
      "Train loss: 0.25097\n",
      "Test loss: 1.13232\n",
      "Train loss: 0.237115\n",
      "Test loss: 1.23633\n",
      "Train loss: 0.253721\n",
      "Test loss: 1.12266\n",
      "Train loss: 0.286209\n",
      "Test loss: 1.0124\n",
      "Train loss: 0.238809\n",
      "Test loss: 1.22362\n",
      "Train loss: 0.233029\n",
      "Test loss: 1.0435\n",
      "Train loss: 0.285606\n",
      "Test loss: 1.20976\n",
      "Train loss: 0.249117\n",
      "Test loss: 1.17401\n",
      "Train loss: 0.235692\n",
      "Test loss: 0.973515\n",
      "Train loss: 0.246669\n",
      "Test loss: 1.26015\n",
      "Train loss: 0.266569\n",
      "Test loss: 1.20025\n",
      "Train loss: 0.272879\n",
      "Test loss: 1.1279\n",
      "Train loss: 0.279453\n",
      "Test loss: 1.19431\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as train_sess:\n",
    "        prediction, logits, loss_op, content_batch_op = make_net('/tmp/train.csv')\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "        global_step = tf.Variable(0,name='global_step', trainable=False)\n",
    "        train_op = optimizer.minimize(loss_op,global_step=global_step)\n",
    "\n",
    "        init_vars_op = tf.global_variables_initializer()\n",
    "        init_tables_op = tf.tables_initializer()\n",
    "\n",
    "\n",
    "        summary_writer = tf.summary.FileWriter('/tmp/tf_tutorial/logs/train1', train_sess.graph)\n",
    "        loss_summary = tf.summary.scalar('loss', loss_op)\n",
    "        merge_summaries_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "        train_sess.run([init_vars_op, init_tables_op])\n",
    "\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        for i in range(TRAIN_ITERATIONS):\n",
    "            train_loss,_, merged_summary, current_global_step = train_sess.run([loss_op, train_op, merge_summaries_op, global_step])\n",
    "            summary_writer.add_summary(merged_summary,current_global_step)\n",
    "            print \"Train loss: %s\" % train_loss\n",
    "            \n",
    "            test_loss = train_sess.run(loss_op, feed_dict={content_batch_op: test_batch})\n",
    "            test_summary = tf.Summary(\n",
    "                value=[tf.Summary.Value(tag=\"test_loss\", simple_value=test_loss)])\n",
    "            summary_writer.add_summary(test_summary, current_global_step)\n",
    "\n",
    "            print \"Test loss: %s\" % test_loss\n",
    "        \n",
    "        summary_writer.flush()    \n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "cmd = 'tensorboard --logdir /tmp/tf_tutorial/logs'\n",
    "# Run our docker\n",
    "p = subprocess.Popen(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
