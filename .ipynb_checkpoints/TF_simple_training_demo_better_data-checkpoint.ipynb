{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change query to make it larger and stratify sampling via window function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s=r'''\n",
    "SELECT suffix, processed_content, RAND() >0.1 as is_train\n",
    "FROM (\n",
    "  SELECT \n",
    "          REGEXP_EXTRACT(sample_path, '.*(\\\\..+)$') as suffix,\n",
    "          substr(content,1,1024)  as processed_content,\n",
    "          RAND() AS rnd, \n",
    "          ROW_NUMBER() OVER(PARTITION BY suffix ORDER BY rnd) AS pos\n",
    "        FROM  [bigquery-public-data:github_repos.sample_contents]\n",
    "        WHERE \n",
    "            LENGTH(content) >= 1024\n",
    "            AND content IS NOT NULL\n",
    "            AND content != ''\n",
    "            AND REGEXP_EXTRACT(sample_path, '.*(\\\\..+)$') in ('.py','.c','.rb')\n",
    "            AND REGEXP_MATCH(sample_path, '.*(\\\\..+)$')\n",
    "        ) a\n",
    "     WHERE \n",
    "        pos <= 5000/3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "c = bigquery.Client()\n",
    "query = c.run_sync_query(s)\n",
    "query.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data to 2 csvs according to random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from base64 import urlsafe_b64encode\n",
    "\n",
    "\n",
    "def transform_content(content):\n",
    "    content = content.encode('utf-8')    \n",
    "    content = content[:1024]    \n",
    "    return content\n",
    "\n",
    "with file('/tmp/train.csv','wb') as train_out_file:\n",
    "    with file('/tmp/test.csv','wb') as test_out_file:\n",
    "        w_train = csv.writer(train_out_file, quoting=csv.QUOTE_ALL, delimiter='\\t')\n",
    "        w_test = csv.writer(test_out_file, quoting=csv.QUOTE_ALL, delimiter='\\t')\n",
    "        for language,content, is_train in query.rows:\n",
    "            print language, is_train\n",
    "            w = w_train if is_train else w_test\n",
    "            w.writerow([language,urlsafe_b64encode(transform_content(content))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packaged net building into method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_ITERATIONS = 500\n",
    "MAX_STRING_SIZE = 1024\n",
    "\n",
    "def make_net(input_path, batch_size=BATCH_SIZE):\n",
    "    filenames_queue = tf.train.string_input_producer([input_path])\n",
    "    reader = tf.TextLineReader()\n",
    "    key, value = reader.read(filenames_queue)\n",
    "    default_values=[['UNKNOWN'], ['']]\n",
    "\n",
    "\n",
    "    # decode content\n",
    "    language, b64_content = tf.decode_csv(value,default_values, field_delim='\\t')\n",
    "    content = tf.decode_base64(b64_content)\n",
    "\n",
    "    language_batch_op, content_batch_op = tf.train.shuffle_batch([language,content], \n",
    "                                                                 batch_size=batch_size, \n",
    "                                                                 capacity=1000, \n",
    "                                                                 min_after_dequeue=100)\n",
    "    #Make hash table for langauges\n",
    "    language_keys=['.py','.c','.h']\n",
    "    values=range(1, len(language_keys)+1)\n",
    "    language_codes_table = tf.contrib.lookup.HashTable(\n",
    "        tf.contrib.lookup.KeyValueTensorInitializer(language_keys, values), 0)\n",
    "\n",
    "    #Make embeddings for the characters\n",
    "    bytes = tf.transpose(tf.decode_raw(content_batch_op, tf.uint8))\n",
    "    bytes_embedding_weights = tf.Variable(name=\"embedding_weights\",\n",
    "                                          initial_value=tf.random_uniform(shape=(256, 64),\n",
    "                                                                          minval=-0.1, \n",
    "                                                                          maxval=0.1))\n",
    "    bytes_embedding = tf.nn.embedding_lookup(bytes_embedding_weights, tf.cast(bytes,tf.int32))\n",
    "    embedding_mean = tf.reduce_mean(bytes_embedding,axis=0)\n",
    "\n",
    "    # Convert languages to numeric codes\n",
    "    language_codes_indices = language_codes_table.lookup(language_batch_op)\n",
    "    language_codes_batch_op = tf.one_hot(language_codes_indices, len(language_keys))\n",
    "    dense_weights = tf.get_variable(name='dense_weights',\n",
    "                              shape=[64, len(language_keys)],\n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.Variable(tf.zeros([len(language_keys)]), name='biases')\n",
    "    logits = tf.nn.relu(tf.matmul(embedding_mean, dense_weights) + biases, name='logits')\n",
    "    prediction = tf.argmax(logits, 1)\n",
    "\n",
    "    \n",
    "    batch_loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=language_codes_batch_op)\n",
    "    loss_op = tf.reduce_mean(batch_loss, name='loss')\n",
    "    return prediction, logits, loss_op, content_batch_op\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test batch by running content batch op only on test csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    _,_,_, content_batch_op = make_net('/tmp/test.csv')\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    test_batch = sess.run([content_batch_op])[0] \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train as usual but add summaries for both train and test, where for test only run the content batch op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as train_sess:\n",
    "        prediction, logits, loss_op, content_batch_op = make_net('/tmp/train.csv')\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "        global_step = tf.Variable(0,name='global_step', trainable=False)\n",
    "        train_op = optimizer.minimize(loss_op,global_step=global_step)\n",
    "\n",
    "        init_vars_op = tf.global_variables_initializer()\n",
    "        init_tables_op = tf.tables_initializer()\n",
    "\n",
    "\n",
    "        summary_writer = tf.summary.FileWriter('/tmp/tf_tutorial/logs/train2', train_sess.graph)\n",
    "        loss_summary = tf.summary.scalar('loss', loss_op)\n",
    "        merge_summaries_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "        train_sess.run([init_vars_op, init_tables_op])\n",
    "\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        for i in range(TRAIN_ITERATIONS):\n",
    "            train_loss,_, merged_summary, current_global_step = train_sess.run([loss_op, train_op, merge_summaries_op, global_step])\n",
    "            summary_writer.add_summary(merged_summary,current_global_step)\n",
    "            print \"Train loss: %s\" % train_loss\n",
    "            \n",
    "            test_loss = train_sess.run(loss_op, feed_dict={content_batch_op: test_batch})\n",
    "            test_summary = tf.Summary(\n",
    "                value=[tf.Summary.Value(tag=\"test_loss\", simple_value=test_loss)])\n",
    "            summary_writer.add_summary(test_summary, current_global_step)\n",
    "\n",
    "            print \"Test loss: %s\" % test_loss\n",
    "        \n",
    "        summary_writer.flush()    \n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model import signature_constants\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "from tensorflow.python.saved_model import signature_def_utils\n",
    "from tensorflow.python.saved_model import utils\n",
    "\n",
    "! rm -rf /tmp/tf_tutorial/model/0\n",
    "\n",
    "with sess.as_default():    \n",
    "    legacy_init_op = tf.group(\n",
    "          tf.tables_initializer(), name='legacy_init_op')\n",
    "    builder = saved_model_builder.SavedModelBuilder('/tmp/tf_tutorial/model/0')\n",
    "    signature = signature_def_utils.build_signature_def(\n",
    "          inputs={'content': utils.build_tensor_info(content_batch_op)},\n",
    "          outputs={'logits': utils.build_tensor_info(logits),\n",
    "                   'prediction': utils.build_tensor_info(prediction)},\n",
    "          method_name=signature_constants.PREDICT_METHOD_NAME)    \n",
    "    builder.add_meta_graph_and_variables(\n",
    "                        sess, \n",
    "                        [tag_constants.SERVING],\n",
    "                        signature_def_map={\n",
    "                            'predict_language': signature,\n",
    "                        },\n",
    "                        main_op=legacy_init_op\n",
    "                        )                \n",
    "    builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
